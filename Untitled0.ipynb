{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallasathena92/NN/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "k489TbNrx8OP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **1. Data preparation**"
      ]
    },
    {
      "metadata": {
        "id": "yiKxxYlUssnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "2145a237-431d-4ac1-c7aa-cfbc7c7d6156"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m2EDP7Ctsukz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8450f32b-34ed-48cb-b13b-48ded88e71b7"
      },
      "cell_type": "code",
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    \n",
        "    results =  np.zeros((len(y), num_class))\n",
        "    for i, label in enumerate(y):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5DKLRe2Osy3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c8ad2414-7a95-461d-8c9b-7511738fdd21"
      },
      "cell_type": "code",
      "source": [
        "rand_indices = np.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9ZUna7T6wnfa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **2. Build a CNN and tune its hyper-parameters**"
      ]
    },
    {
      "metadata": {
        "id": "STnBszozs-xY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range = 40,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LAmrqh-_tAg1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow(\n",
        "    x_tr,\n",
        "    y_tr,\n",
        "    batch_size = 32\n",
        ")\n",
        "valid_generator = train_datagen.flow(\n",
        "    x_val,\n",
        "    y_val,\n",
        "    batch_size = 32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z-pXsi-4tBzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1267
        },
        "outputId": "be07c9e3-e276-48f8-983d-994c63069eca"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout,BatchNormalization,Activation,ZeroPadding2D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation = 'relu',padding='same', \n",
        "                 input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32,(3,3),padding = 'same'))\n",
        "model.add(BatchNormalization ())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(48, (3, 3),activation = 'relu',padding='same'))\n",
        "model.add(Conv2D(48,(3,3),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu',padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu',padding='same'))\n",
        "model.add(Conv2D(64,(3,3),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(80, (3, 3), activation = 'relu',padding='same'))\n",
        "model.add(Conv2D(80, (3, 3), activation = 'relu',padding='same'))\n",
        "model.add(Conv2D(80,(3,3),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation = 'relu',padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation = 'relu',padding='same'))\n",
        "model.add(Conv2D(128,(3,3),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 16, 16, 48)        13872     \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 16, 16, 48)        20784     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 16, 16, 48)        192       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16, 16, 48)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 48)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 8, 8, 64)          27712     \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 4, 4, 80)          46160     \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 4, 4, 80)          57680     \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 4, 4, 80)          57680     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 4, 4, 80)          320       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 4, 4, 80)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 80)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 2, 2, 128)         92288     \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 2, 2, 128)         147584    \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 2, 2, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 2, 2, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 732,346\n",
            "Trainable params: 731,642\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xf-ZYBvltFka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 2E-4 # to be tuned!\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGFHq3W7tI1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3537
        },
        "outputId": "6627f22a-6d61-43df-f7c9-473a0210bb2e"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=x_tr.shape[0] // 32,\n",
        "        epochs=100,\n",
        "        validation_data=valid_generator,\n",
        "        validation_steps=800)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 56s 44ms/step - loss: 1.8878 - acc: 0.3060 - val_loss: 1.6586 - val_acc: 0.3966\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.6075 - acc: 0.4163 - val_loss: 1.5860 - val_acc: 0.4205\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 54s 44ms/step - loss: 1.4803 - acc: 0.4680 - val_loss: 1.4478 - val_acc: 0.4685\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 53s 43ms/step - loss: 1.4073 - acc: 0.4952 - val_loss: 1.4070 - val_acc: 0.4975\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.3381 - acc: 0.5240 - val_loss: 1.7551 - val_acc: 0.4321\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.2775 - acc: 0.5494 - val_loss: 1.7377 - val_acc: 0.4370\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.2348 - acc: 0.5642 - val_loss: 1.3260 - val_acc: 0.5391\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 53s 43ms/step - loss: 1.1986 - acc: 0.5794 - val_loss: 1.2814 - val_acc: 0.5545\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.1633 - acc: 0.5934 - val_loss: 1.2416 - val_acc: 0.5617\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.1328 - acc: 0.6086 - val_loss: 1.2336 - val_acc: 0.5770\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 1.1057 - acc: 0.6130 - val_loss: 1.1460 - val_acc: 0.5981\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.0910 - acc: 0.6281 - val_loss: 1.1901 - val_acc: 0.5779\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.0667 - acc: 0.6314 - val_loss: 1.1883 - val_acc: 0.5833\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.0352 - acc: 0.6431 - val_loss: 1.0485 - val_acc: 0.6386\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 1.0242 - acc: 0.6489 - val_loss: 1.1832 - val_acc: 0.6010\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 53s 43ms/step - loss: 1.0097 - acc: 0.6542 - val_loss: 1.0349 - val_acc: 0.6387\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 53s 43ms/step - loss: 0.9854 - acc: 0.6630 - val_loss: 0.9279 - val_acc: 0.6782\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 0.9731 - acc: 0.6694 - val_loss: 0.9005 - val_acc: 0.6851\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 0.9647 - acc: 0.6724 - val_loss: 0.9349 - val_acc: 0.6680\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 0.9433 - acc: 0.6785 - val_loss: 0.9507 - val_acc: 0.6668\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 0.9374 - acc: 0.6835 - val_loss: 0.9524 - val_acc: 0.6658\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 53s 43ms/step - loss: 0.9249 - acc: 0.6871 - val_loss: 0.9130 - val_acc: 0.6864\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 53s 43ms/step - loss: 0.9127 - acc: 0.6907 - val_loss: 1.0162 - val_acc: 0.6506\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 55s 44ms/step - loss: 0.8990 - acc: 0.6977 - val_loss: 0.9567 - val_acc: 0.6703\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 53s 43ms/step - loss: 0.8968 - acc: 0.6964 - val_loss: 0.8826 - val_acc: 0.6964\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 0.8784 - acc: 0.7056 - val_loss: 0.8473 - val_acc: 0.7132\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.8734 - acc: 0.7064 - val_loss: 0.9218 - val_acc: 0.6841\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.8674 - acc: 0.7069 - val_loss: 0.8183 - val_acc: 0.7194\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.8545 - acc: 0.7127 - val_loss: 0.8087 - val_acc: 0.7195\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 54s 43ms/step - loss: 0.8531 - acc: 0.7117 - val_loss: 0.7912 - val_acc: 0.7248\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.8352 - acc: 0.7207 - val_loss: 0.7771 - val_acc: 0.7367\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 0.8322 - acc: 0.7212 - val_loss: 1.0571 - val_acc: 0.6753\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.8243 - acc: 0.7239 - val_loss: 0.7878 - val_acc: 0.7318\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.8282 - acc: 0.7244 - val_loss: 0.7895 - val_acc: 0.7286\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.8056 - acc: 0.7300 - val_loss: 0.7550 - val_acc: 0.7372\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.8037 - acc: 0.7315 - val_loss: 0.8166 - val_acc: 0.7182\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.8010 - acc: 0.7341 - val_loss: 0.7432 - val_acc: 0.7472\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 52s 42ms/step - loss: 0.8001 - acc: 0.7323 - val_loss: 0.7715 - val_acc: 0.7313\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 0.7920 - acc: 0.7370 - val_loss: 0.7311 - val_acc: 0.7501\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.7845 - acc: 0.7390 - val_loss: 0.7643 - val_acc: 0.7432\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.7833 - acc: 0.7408 - val_loss: 0.7909 - val_acc: 0.7286\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.7729 - acc: 0.7452 - val_loss: 0.8908 - val_acc: 0.7083\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.7625 - acc: 0.7465 - val_loss: 0.8112 - val_acc: 0.7268\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 0.7674 - acc: 0.7431 - val_loss: 0.7985 - val_acc: 0.7313\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.7581 - acc: 0.7488 - val_loss: 0.7622 - val_acc: 0.7403\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.7517 - acc: 0.7494 - val_loss: 0.7182 - val_acc: 0.7563\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.7535 - acc: 0.7519 - val_loss: 0.6926 - val_acc: 0.7648\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.7406 - acc: 0.7528 - val_loss: 0.9599 - val_acc: 0.6974\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 51s 40ms/step - loss: 0.7355 - acc: 0.7561 - val_loss: 0.8280 - val_acc: 0.7258\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.7340 - acc: 0.7563 - val_loss: 0.7533 - val_acc: 0.7433\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.7283 - acc: 0.7596 - val_loss: 0.7989 - val_acc: 0.7355\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.7242 - acc: 0.7598 - val_loss: 0.7101 - val_acc: 0.7619\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.7138 - acc: 0.7626 - val_loss: 0.7294 - val_acc: 0.7529\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.7246 - acc: 0.7608 - val_loss: 0.7204 - val_acc: 0.7549\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.7147 - acc: 0.7638 - val_loss: 0.6985 - val_acc: 0.7666\n",
            "Epoch 56/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.7055 - acc: 0.7681 - val_loss: 0.7118 - val_acc: 0.7603\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.7065 - acc: 0.7674 - val_loss: 0.6795 - val_acc: 0.7662\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.7015 - acc: 0.7679 - val_loss: 0.6666 - val_acc: 0.7732\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.7010 - acc: 0.7694 - val_loss: 0.6559 - val_acc: 0.7784\n",
            "Epoch 60/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.7004 - acc: 0.7670 - val_loss: 0.7267 - val_acc: 0.7580\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6896 - acc: 0.7710 - val_loss: 0.6438 - val_acc: 0.7785\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - 49s 40ms/step - loss: 0.6872 - acc: 0.7723 - val_loss: 0.6914 - val_acc: 0.7711\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.6869 - acc: 0.7737 - val_loss: 0.6397 - val_acc: 0.7842\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6888 - acc: 0.7697 - val_loss: 0.6633 - val_acc: 0.7756\n",
            "Epoch 65/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6829 - acc: 0.7761 - val_loss: 0.7327 - val_acc: 0.7559\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6760 - acc: 0.7771 - val_loss: 0.6120 - val_acc: 0.7916\n",
            "Epoch 67/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6850 - acc: 0.7733 - val_loss: 0.6885 - val_acc: 0.7669\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6799 - acc: 0.7739 - val_loss: 0.8528 - val_acc: 0.7350\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.6712 - acc: 0.7812 - val_loss: 0.6603 - val_acc: 0.7749\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6736 - acc: 0.7770 - val_loss: 0.7603 - val_acc: 0.7458\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6682 - acc: 0.7820 - val_loss: 0.6240 - val_acc: 0.7916\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6702 - acc: 0.7782 - val_loss: 0.6446 - val_acc: 0.7825\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6621 - acc: 0.7833 - val_loss: 0.6236 - val_acc: 0.7930\n",
            "Epoch 74/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.6607 - acc: 0.7841 - val_loss: 0.6091 - val_acc: 0.7918\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6601 - acc: 0.7838 - val_loss: 0.6110 - val_acc: 0.7930\n",
            "Epoch 76/100\n",
            "1250/1250 [==============================] - 51s 40ms/step - loss: 0.6546 - acc: 0.7843 - val_loss: 0.6412 - val_acc: 0.7886\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - 49s 40ms/step - loss: 0.6486 - acc: 0.7876 - val_loss: 0.6377 - val_acc: 0.7870\n",
            "Epoch 78/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6628 - acc: 0.7831 - val_loss: 0.6011 - val_acc: 0.7974\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6502 - acc: 0.7875 - val_loss: 0.6495 - val_acc: 0.7761\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6452 - acc: 0.7893 - val_loss: 0.6098 - val_acc: 0.7982\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - 49s 40ms/step - loss: 0.6531 - acc: 0.7867 - val_loss: 0.6184 - val_acc: 0.7868\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.6492 - acc: 0.7893 - val_loss: 0.7433 - val_acc: 0.7630\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6447 - acc: 0.7875 - val_loss: 0.6380 - val_acc: 0.7875\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6516 - acc: 0.7878 - val_loss: 0.6367 - val_acc: 0.7871\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6393 - acc: 0.7917 - val_loss: 0.6359 - val_acc: 0.7857\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6399 - acc: 0.7906 - val_loss: 0.7027 - val_acc: 0.7698\n",
            "Epoch 87/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6386 - acc: 0.7899 - val_loss: 0.6241 - val_acc: 0.7856\n",
            "Epoch 88/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 0.6424 - acc: 0.7902 - val_loss: 0.6064 - val_acc: 0.7979\n",
            "Epoch 89/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6414 - acc: 0.7913 - val_loss: 0.5733 - val_acc: 0.8037\n",
            "Epoch 90/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6406 - acc: 0.7911 - val_loss: 0.5657 - val_acc: 0.8084\n",
            "Epoch 91/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.6378 - acc: 0.7928 - val_loss: 0.5886 - val_acc: 0.8018\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6279 - acc: 0.7965 - val_loss: 0.7252 - val_acc: 0.7574\n",
            "Epoch 93/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6403 - acc: 0.7919 - val_loss: 0.5692 - val_acc: 0.8091\n",
            "Epoch 94/100\n",
            "1250/1250 [==============================] - 51s 41ms/step - loss: 0.6285 - acc: 0.7960 - val_loss: 0.5996 - val_acc: 0.8019\n",
            "Epoch 95/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6304 - acc: 0.7981 - val_loss: 0.6206 - val_acc: 0.7912\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6312 - acc: 0.7957 - val_loss: 0.5807 - val_acc: 0.8067\n",
            "Epoch 97/100\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.6255 - acc: 0.7973 - val_loss: 0.5968 - val_acc: 0.8027\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6359 - acc: 0.7934 - val_loss: 0.6657 - val_acc: 0.7801\n",
            "Epoch 99/100\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.6224 - acc: 0.8002 - val_loss: 0.6029 - val_acc: 0.8035\n",
            "Epoch 100/100\n",
            "1250/1250 [==============================] - 52s 41ms/step - loss: 0.6278 - acc: 0.7955 - val_loss: 0.6566 - val_acc: 0.7822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cSB6D_TmxPU8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "9251dfc8-da5e-44d4-c9da-30c6eef6807d"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nASFbsdLwRXoMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Mo4Q2fR_tKV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rDEBsf4eyCzQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **3. Train (again) and evaluate the model**"
      ]
    },
    {
      "metadata": {
        "id": "-_jthebvyJg7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3.1. Train the model on the entire training set**"
      ]
    },
    {
      "metadata": {
        "id": "fgDrjJRC7rB7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 2E-4 \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QOJsg14_yGjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3537
        },
        "outputId": "71bc0cc0-ff66-45a9-b7cf-fd30e21de676"
      },
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow(\n",
        "    x_train,\n",
        "    y_train_vec,\n",
        "    batch_size = 32\n",
        ")\n",
        "history = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=x_train.shape[0] // 32,\n",
        "        epochs=100\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1562/1562 [==============================] - 48s 31ms/step - loss: 0.6281 - acc: 0.7981\n",
            "Epoch 2/100\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 0.6269 - acc: 0.7993\n",
            "Epoch 3/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.6292 - acc: 0.7984\n",
            "Epoch 4/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.6192 - acc: 0.8001\n",
            "Epoch 5/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6283 - acc: 0.7953\n",
            "Epoch 6/100\n",
            "1562/1562 [==============================] - 47s 30ms/step - loss: 0.6214 - acc: 0.8011\n",
            "Epoch 7/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6237 - acc: 0.8014\n",
            "Epoch 8/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6208 - acc: 0.8011\n",
            "Epoch 9/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6199 - acc: 0.8022\n",
            "Epoch 10/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6231 - acc: 0.8025\n",
            "Epoch 11/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6234 - acc: 0.8002\n",
            "Epoch 12/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6160 - acc: 0.8024\n",
            "Epoch 13/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.6234 - acc: 0.8025\n",
            "Epoch 14/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6279 - acc: 0.8002\n",
            "Epoch 15/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.6165 - acc: 0.8030\n",
            "Epoch 16/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6200 - acc: 0.8020\n",
            "Epoch 17/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6083 - acc: 0.8060\n",
            "Epoch 18/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.6233 - acc: 0.8025\n",
            "Epoch 19/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6164 - acc: 0.8066\n",
            "Epoch 20/100\n",
            "1562/1562 [==============================] - 46s 30ms/step - loss: 0.6244 - acc: 0.8033\n",
            "Epoch 21/100\n",
            "1562/1562 [==============================] - 45s 28ms/step - loss: 0.6251 - acc: 0.7995\n",
            "Epoch 22/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6241 - acc: 0.8027\n",
            "Epoch 23/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6252 - acc: 0.8013\n",
            "Epoch 24/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6295 - acc: 0.8021\n",
            "Epoch 25/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6247 - acc: 0.8029\n",
            "Epoch 26/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6353 - acc: 0.8043\n",
            "Epoch 27/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.6438 - acc: 0.8015\n",
            "Epoch 28/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6289 - acc: 0.8022\n",
            "Epoch 29/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6431 - acc: 0.8024\n",
            "Epoch 30/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6405 - acc: 0.8027\n",
            "Epoch 31/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6188 - acc: 0.8068\n",
            "Epoch 32/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6352 - acc: 0.8039\n",
            "Epoch 33/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6326 - acc: 0.8064\n",
            "Epoch 34/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.6406 - acc: 0.8036\n",
            "Epoch 35/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6213 - acc: 0.8086\n",
            "Epoch 36/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6459 - acc: 0.8017\n",
            "Epoch 37/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6276 - acc: 0.8068\n",
            "Epoch 38/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6297 - acc: 0.8080\n",
            "Epoch 39/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6417 - acc: 0.8070\n",
            "Epoch 40/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6471 - acc: 0.8036\n",
            "Epoch 41/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6385 - acc: 0.8070\n",
            "Epoch 42/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6331 - acc: 0.8062\n",
            "Epoch 43/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.6534 - acc: 0.8038\n",
            "Epoch 44/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6388 - acc: 0.8046\n",
            "Epoch 45/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6540 - acc: 0.8064\n",
            "Epoch 46/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6189 - acc: 0.8097\n",
            "Epoch 47/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6298 - acc: 0.8100\n",
            "Epoch 48/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6518 - acc: 0.8063\n",
            "Epoch 49/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6528 - acc: 0.8053\n",
            "Epoch 50/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6435 - acc: 0.8075\n",
            "Epoch 51/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6530 - acc: 0.8095\n",
            "Epoch 52/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.6660 - acc: 0.8059\n",
            "Epoch 53/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6583 - acc: 0.8056\n",
            "Epoch 54/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6762 - acc: 0.8062\n",
            "Epoch 55/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6621 - acc: 0.8062\n",
            "Epoch 56/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6664 - acc: 0.8063\n",
            "Epoch 57/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6585 - acc: 0.8086\n",
            "Epoch 58/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6591 - acc: 0.8063\n",
            "Epoch 59/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6675 - acc: 0.8064\n",
            "Epoch 60/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6783 - acc: 0.8090\n",
            "Epoch 61/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6946 - acc: 0.8059\n",
            "Epoch 62/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6601 - acc: 0.8090\n",
            "Epoch 63/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6928 - acc: 0.8072\n",
            "Epoch 64/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6881 - acc: 0.8091\n",
            "Epoch 65/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6837 - acc: 0.8076\n",
            "Epoch 66/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6928 - acc: 0.8069\n",
            "Epoch 67/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6944 - acc: 0.8071\n",
            "Epoch 68/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6909 - acc: 0.8061\n",
            "Epoch 69/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.7216 - acc: 0.8051\n",
            "Epoch 70/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6958 - acc: 0.8064\n",
            "Epoch 71/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6699 - acc: 0.8093\n",
            "Epoch 72/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6919 - acc: 0.8078\n",
            "Epoch 73/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7215 - acc: 0.8057\n",
            "Epoch 74/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6924 - acc: 0.8070\n",
            "Epoch 75/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6950 - acc: 0.8076\n",
            "Epoch 76/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6781 - acc: 0.8096\n",
            "Epoch 77/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6922 - acc: 0.8089\n",
            "Epoch 78/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.7136 - acc: 0.8076\n",
            "Epoch 79/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7041 - acc: 0.8076\n",
            "Epoch 80/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6787 - acc: 0.8106\n",
            "Epoch 81/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.7012 - acc: 0.8095\n",
            "Epoch 82/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6718 - acc: 0.8106\n",
            "Epoch 83/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6779 - acc: 0.8106\n",
            "Epoch 84/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6869 - acc: 0.8122\n",
            "Epoch 85/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6910 - acc: 0.8089\n",
            "Epoch 86/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6714 - acc: 0.8112\n",
            "Epoch 87/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.7096 - acc: 0.8086\n",
            "Epoch 88/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6807 - acc: 0.8135\n",
            "Epoch 89/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6728 - acc: 0.8141\n",
            "Epoch 90/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6585 - acc: 0.8170\n",
            "Epoch 91/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6655 - acc: 0.8142\n",
            "Epoch 92/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6676 - acc: 0.8120\n",
            "Epoch 93/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6637 - acc: 0.8122\n",
            "Epoch 94/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6515 - acc: 0.8166\n",
            "Epoch 95/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6640 - acc: 0.8130\n",
            "Epoch 96/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6526 - acc: 0.8143\n",
            "Epoch 97/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.6556 - acc: 0.8171\n",
            "Epoch 98/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6529 - acc: 0.8143\n",
            "Epoch 99/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.6480 - acc: 0.8188\n",
            "Epoch 100/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6533 - acc: 0.8174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JlOdMqfobqCS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save(\"final_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xEBhNQFAyHVS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3.2. Evaluate the model on the test set**"
      ]
    },
    {
      "metadata": {
        "id": "oVd7outaHhya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "85e7cc49-c8cc-4267-c924-a0dac2885126"
      },
      "cell_type": "code",
      "source": [
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 238us/step\n",
            "loss = 0.5661867559671402\n",
            "accuracy = 0.8338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0yHgoe-xIxH8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}